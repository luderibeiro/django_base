name: âš¡ Performance Testing

on:
    push:
        branches: [main, develop]
    pull_request:
        branches: [main]
    schedule:
        # Run weekly on Sunday at 3 AM UTC
        - cron: '0 3 * * 0'
    workflow_dispatch:

permissions:
    contents: read

jobs:
    performance-test:
        name: âš¡ Performance Tests
        runs-on: ubuntu-latest

        services:
            postgres:
                image: postgres:13
                env:
                    POSTGRES_PASSWORD: postgres
                    POSTGRES_DB: test_db
                options: >-
                    --health-cmd pg_isready
                    --health-interval 10s
                    --health-timeout 5s
                    --health-retries 5
                ports:
                    - 5432:5432

            redis:
                image: redis:6
                options: >-
                    --health-cmd "redis-cli ping"
                    --health-interval 10s
                    --health-timeout 5s
                    --health-retries 5
                ports:
                    - 6379:6379

        steps:
            - name: ðŸ“¥ Checkout code
              uses: actions/checkout@v4

            - name: ðŸ Set up Python
              uses: actions/setup-python@v4
              with:
                  python-version: "3.12"
                  cache: "pip"

            - name: ðŸ“¦ Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r project/requirements.txt
                  pip install pytest-benchmark locust memory-profiler

            - name: ðŸ§ª Run unit performance tests
              env:
                  DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
                  REDIS_URL: redis://localhost:6379/0
                  SECRET_KEY: ${{ secrets.SECRET_KEY_FOR_CI }}
                  DEBUG: False
                  OAUTH2_CLIENT_ID: test-client-id
                  OAUTH2_CLIENT_SECRET: test-client-secret
              run: |
                  cd project
                  export PYTHONPATH=$PWD
                  pytest --benchmark-only --benchmark-save=performance --benchmark-save-data -v

            - name: ðŸ“Š Generate performance report
              run: |
                  cd project
                  python -c "
                  import json
                  import os
                  import glob
                  
                  # Find benchmark files
                  benchmark_files = glob.glob('.benchmarks/*/*.json')
                  
                  if benchmark_files:
                      with open(benchmark_files[0], 'r') as f:
                          data = json.load(f)
                      
                      # Extract performance metrics
                      benchmarks = data.get('benchmarks', [])
                      total_tests = len(benchmarks)
                      slow_tests = [b for b in benchmarks if b.get('stats', {}).get('mean', 0) > 1.0]  # > 1 second
                      
                      summary = {
                          'total_benchmarks': total_tests,
                          'slow_tests': len(slow_tests),
                          'slow_test_names': [b.get('name', 'unknown') for b in slow_tests],
                          'average_time': sum(b.get('stats', {}).get('mean', 0) for b in benchmarks) / total_tests if total_tests > 0 else 0,
                          'timestamp': os.popen('date -u +%Y-%m-%dT%H:%M:%SZ').read().strip()
                      }
                      
                      with open('performance_summary.json', 'w') as f:
                          json.dump(summary, f, indent=2)
                      
                      print('Performance Summary:')
                      print(json.dumps(summary, indent=2))
                  else:
                      print('No benchmark files found')
                  "

            - name: ðŸš€ Run load tests with Locust
              run: |
                  cd project
                  # Create a simple locustfile for testing
                  cat > locustfile.py << 'EOF'
                  from locust import HttpUser, task, between
                  
                  class WebsiteUser(HttpUser):
                      wait_time = between(1, 3)
                      
                      def on_start(self):
                          # Login and get token
                          response = self.client.post("/api/v1/auth/login/", json={
                              "username": "admin",
                              "password": "admin"
                          })
                          if response.status_code == 200:
                              self.token = response.json().get("access_token")
                              self.client.headers.update({"Authorization": f"Bearer {self.token}"})
                      
                      @task(3)
                      def get_users(self):
                          self.client.get("/api/v1/users/")
                      
                      @task(1)
                      def get_health(self):
                          self.client.get("/health/")
                  EOF
                  
                  # Run locust for 2 minutes with 10 users
                  locust -f locustfile.py --host=http://localhost:8000 --users=10 --spawn-rate=2 --run-time=2m --headless --html=locust_report.html --csv=locust_report

            - name: ðŸ§  Memory profiling
              run: |
                  cd project
                  python -c "
                  import memory_profiler
                  import subprocess
                  import os
                  
                  # Profile memory usage during Django startup
                  @memory_profiler.profile
                  def run_django_check():
                      os.environ['DJANGO_SETTINGS_MODULE'] = 'project.settings'
                      import django
                      django.setup()
                      from django.core.management import execute_from_command_line
                      execute_from_command_line(['manage.py', 'check'])
                  
                  try:
                      run_django_check()
                  except Exception as e:
                      print(f'Memory profiling completed with warnings: {e}')
                  "

            - name: ðŸ“Š Generate comprehensive performance report
              run: |
                  python -c "
                  import json
                  import os
                  import glob
                  
                  # Collect all performance data
                  performance_data = {
                      'timestamp': os.popen('date -u +%Y-%m-%dT%H:%M:%SZ').read().strip(),
                      'benchmarks': {},
                      'load_tests': {},
                      'memory_usage': {}
                  }
                  
                  # Load benchmark data
                  try:
                      with open('project/performance_summary.json', 'r') as f:
                          performance_data['benchmarks'] = json.load(f)
                  except FileNotFoundError:
                      performance_data['benchmarks'] = {'error': 'No benchmark data found'}
                  
                  # Load locust data
                  try:
                      with open('project/locust_report_stats.csv', 'r') as f:
                          lines = f.readlines()
                          if len(lines) > 1:
                              # Parse CSV data
                              headers = lines[0].strip().split(',')
                              data = lines[1].strip().split(',')
                              performance_data['load_tests'] = dict(zip(headers, data))
                  except FileNotFoundError:
                      performance_data['load_tests'] = {'error': 'No load test data found'}
                  
                  # Save comprehensive report
                  with open('comprehensive_performance_report.json', 'w') as f:
                      json.dump(performance_data, f, indent=2)
                  
                  print('Comprehensive Performance Report:')
                  print(json.dumps(performance_data, indent=2))
                  "

            - name: ðŸ“¤ Upload performance reports
              uses: actions/upload-artifact@v4
              with:
                  name: performance-reports
                  path: |
                      project/.benchmarks/
                      project/performance_summary.json
                      project/locust_report.html
                      project/locust_report*.csv
                      comprehensive_performance_report.json

            - name: ðŸš¨ Create performance issue if degradation detected
              if: failure()
              uses: actions/github-script@v7
              with:
                  script: |
                      const fs = require('fs');
                      try {
                          const report = JSON.parse(fs.readFileSync('comprehensive_performance_report.json', 'utf8'));
                          
                          await github.rest.issues.create({
                              owner: context.repo.owner,
                              repo: context.repo.repo,
                              title: `âš¡ Performance degradation detected`,
                              body: `## Performance Alert
                              
                              Performance tests have detected potential performance issues.
                              
                              **Report generated on:** ${report.timestamp}
                              
                              Please review the performance reports and optimize the code.
                              
                              ### Benchmarks
                              \`\`\`json
                              ${JSON.stringify(report.benchmarks, null, 2)}
                              \`\`\`
                              
                              ### Load Tests
                              \`\`\`json
                              ${JSON.stringify(report.load_tests, null, 2)}
                              \`\`\``,
                              labels: ['performance', 'optimization']
                          });
                      } catch (error) {
                          console.log('Could not create performance issue:', error);
                      }
